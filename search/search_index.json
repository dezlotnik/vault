{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to David's Obsidian Vault","text":"<p>For full documentation visit mkdocs.org.</p>"},{"location":"#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"Algorithms%20and%20Data%20Structures/Algorithms%20and%20Data%20Structures/","title":"Algorithms and Data Structures","text":"<p>%% Begin Waypoint %% - Breadth First Search - Graph Search - How To Solve Problems - Majority Element Problem</p> <p>%% End Waypoint %%</p>"},{"location":"Algorithms%20and%20Data%20Structures/Breadth%20First%20Search/","title":"Breadth First Search","text":"<p>The algorithm works as follows:</p> <ol> <li>Start by putting any one of the graph's vertices at the back of a queue.</li> <li>Take the front item of the queue and add it to the visited list.</li> <li>Create a list of that vertex's adjacent nodes. Add the ones which aren't in the visited list to the back of the queue.</li> <li>Keep repeating steps 2 and 3 until the queue is empty.</li> </ol> <pre><code>queue = [start_node]\nvisited = [ ]\n\nwhile (queue):\n    curr_node = queue.pop()\n    visited.append(curr_node)\n    neighbors = getNeighbors(curr_node)\n    for neighbor in neighbors:\n        if (not neighbors in visited):\n            queue.append(neighbor)\n</code></pre>"},{"location":"Algorithms%20and%20Data%20Structures/Graph%20Search/","title":"Graph Search","text":"<p>Breadth First Search [[Depth First Search]] [[Dynamic Programming]]</p>"},{"location":"Algorithms%20and%20Data%20Structures/How%20To%20Solve%20Problems/","title":"How To Solve Problems","text":""},{"location":"Algorithms%20and%20Data%20Structures/How%20To%20Solve%20Problems/#understanding-a-computational-problem","title":"Understanding a Computational Problem","text":"<p>A problem is defined by a set of possible inputs and their relationship to the desired output.</p> <p>Example, given your birthday and the current date, calculate your age in days.</p> <ol> <li>Don't panic!</li> <li>What are the inputs?</li> <li>What are the outputs?</li> <li>Work through some examples by hand. What's the relationship between the inputs and outputs?</li> <li>Try and develop simple mechanical solution (don't optimize prematurely)</li> <li>Develop incrementally, and test as you go</li> </ol>"},{"location":"Algorithms%20and%20Data%20Structures/Majority%20Element%20Problem/","title":"Majority Element Problem","text":"<p>Problem</p> <p>Given an array\u00a0<code>nums</code>\u00a0of size\u00a0<code>n</code>, return\u00a0the majority element. The majority element is the element that appears more than\u00a0<code>\u230an / 2\u230b</code>\u00a0times. You may assume that the majority element always exists in the array.</p>"},{"location":"Algorithms%20and%20Data%20Structures/Majority%20Element%20Problem/#solution-1-map-dictionary","title":"Solution 1: Map / Dictionary","text":"<p>Use a [[Hash Table]] to store the frequency of numbers in the array. Return number with desired frequency.</p> <p>time: O(n) space: O(n)</p>"},{"location":"Algorithms%20and%20Data%20Structures/Majority%20Element%20Problem/#solution-2-voting-algorithm","title":"Solution 2: Voting Algorithm","text":"<p>https://www.topcoder.com/thrive/articles/boyer-moore-majority-vote-algorithm</p>"},{"location":"Optimal%20Control/Dynamic%20Programming%20for%20Optimal%20Control/","title":"Dynamic Programming for Optimal Control","text":"<p>The goal is to solve the following optimization problem,</p> <p></p> <p>The value function, or cost-to-go, is</p> \\[ \\begin{aligned}     J^*(x(t_1),t_1) = \\phi(x^*(t_f),t_f) + \\int_{t_1}^{t_f} \\ell(x^*,u^*, t) \\mathrm{d}t. \\end{aligned} \\] <p>The optimal cost-to-go is the cost incurred from time \\(t_1\\) to \\(t_f\\) assuming optimal controls along that trajectory.  The cost-to-go obeys the Hamilton Jacobi Bellman Equation,</p> <p></p> <p>or alternatively,</p> \\[ \\begin{aligned}     0 = \\text{min}_u \\ \\left ( \\ell(x, u, t) + \\frac{\\partial J^*}{\\partial x} f(x, u, t) + \\frac{\\partial J^*}{\\partial t}(x, t) \\right ). \\end{aligned} \\] <p>The corresponding optimal policy is</p> \\[ \\begin{aligned}     u = \\pi^*(x, t) = \\text{argmin}_u \\left ( \\ell(x, u, t) + \\frac{\\partial J^*}{\\partial x} f(x, u, t) \\right ) \\end{aligned} \\] <p>The optimal control satisfies</p> \\[ \\begin{aligned}     \\frac{\\mathrm{d}J^*}{\\mathrm{d}t}(x,t) = - \\ell(x, u, t) \\end{aligned} \\] <p>Optimal Rate of Change of Cost-to-Go</p> <p>Under the optimal policy, the value function, or cost-to-go, decreases at a rate equal to the cost incurred at the current state. If the current cost is high the value function will decrease rapidly.</p>"},{"location":"Optimal%20Control/Hamilton%20Jacobi%20Bellman%20Equation/","title":"Hamilton Jacobi Bellman Equation","text":""},{"location":"Optimal%20Control/Hamilton%20Jacobi%20Bellman%20Equation/#continuous-state-control","title":"Continuous State &amp; Control","text":"<p>The HJB equation is given by</p> \\[ \\begin{aligned}     \\frac{\\partial J^*}{\\partial t}(x, t) = - \\text{min}_u \\ \\left ( \\ell(x, u, t) + \\frac{\\partial J^*}{\\partial x} f(x, u, t) \\right ) \\end{aligned} \\] <p>^222b91</p>"},{"location":"Optimal%20Control/Hamilton%20Jacobi%20Bellman%20Equation/#discrete-state-actions","title":"Discrete State &amp; Actions","text":"<p>Bellman Equation $$     J^(s_i) = \\text{min}_a \\left [ \\ell(s_i,a) + J^(f(s_i, a)) \\right ] $$</p> <p>Optimal policy</p> \\[ \\begin{aligned}     a = \\pi^*(s_i) = \\text{argmin}_a \\left [  \\ell (s_i,a) + J^*(f(s_i,a)) \\right ] \\end{aligned} \\] <p>The optimal policy satisfies \\(J^*(s_{i+1}) - J^*(s_i) =  - \\ell(s_i,a)\\).</p>"},{"location":"Optimal%20Control/Hamilton%20Jacobi%20Bellman%20Equation/#value-iteration","title":"Value Iteration","text":"<p>How to find the optimal cost-to-go?</p> <p>Idea: start with estimate of the value function, \\(\\hat{J}^*(s_i) = 0\\) for all \\(s_i \\in S\\). Then iterate over</p> \\[     \\hat{J}^*(s_i) = \\text{min}_{a \\in A} [ \\ell(s_i, a) + \\hat{J}^*(f(s_i, a))]. \\]"},{"location":"Optimal%20Control/Optimal%20Control%20Problem%20Statement/","title":"Optimal Control Problem Statement","text":"<p>Dynamic Programming for Optimal Control</p>"},{"location":"Optimal%20Control/Optimal%20Control%20Problem%20Statement/#continuous-time-finite-horizon","title":"Continuous Time Finite Horizon","text":"\\[ \\begin{aligned}     &amp; \\text{min}_u \\ J  = \\phi(x(t_f), t_f) + \\int_{t_0}^{t_f} \\ell(x(t), u(t), t) \\mathrm{d}t \\\\     &amp; \\text{subject to} \\ \\dot{x} = f(x, u, t) \\end{aligned} \\] <p>^9df567</p>"},{"location":"Optimal%20Control/Optimal%20Control%20Problem%20Statement/#discrete-time-finite-horizon","title":"Discrete Time Finite Horizon","text":"\\[ \\begin{aligned}     &amp; \\text{min}_u \\ J  = \\phi(x(N)) + \\sum_{k=0}^{N} \\ell(x(k), u(k)) \\\\     &amp; \\text{subject to} \\ x(k+1) = f(x(k), u(k)) \\end{aligned} \\]"},{"location":"Optimal%20Control/Optimal%20Control/","title":"Optimal Control","text":"<p>%% Begin Waypoint %% - Dynamic Programming for Optimal Control - Hamilton Jacobi Bellman Equation - Optimal Control Problem Statement - References</p> <p>%% End Waypoint %%</p>"},{"location":"Optimal%20Control/References/","title":"References","text":"<p>\"Optimal Control and Estimation\" by Robert F. Stengel</p>"},{"location":"State%20Estimation/Bayes%20Theorem/","title":"Bayes Theorem","text":"<p>Key Idea</p> <p>Bayes theorem provides a way to update a current belief given new information.</p> <p>Bayes theorem is given by, $$ \\begin{aligned}     p(h|e) = \\frac{p(h,e)}{p(e)} = \\frac{p(h) p(e|h)}{p(e)} = \\frac{p(h) p(e|h)}{p(h)p(e|h) + p(\\neg h)p(e|\\neg h)} \\end{aligned} $$</p> <p>In plain English:</p> <p>The probability of the hypothesis, h, given the evidence, e, is equal to the probability of both the hypothesis and the evidence divided by the probability of the evidence.</p> <p>Or,  $$ \\begin{aligned}     p(x|y) = \\frac{p(x,y)}{p(y)} = \\frac{p(x) p(y|x)}{p(y)} \\end{aligned} $$</p> <p>The robot has observed data y. What is the probability that the robot is at state x? This probability is equal to the likelihood of both being at state x and observing data y, divided by the total likelihood of observing y at any state.</p> <ul> <li>\\(p(x)\\) is the prior, probability of being at state \\(x\\) before incorporating the data \\(y\\).</li> <li>\\(p(y | x)\\) is the posterior probability distribution. This is the probability of observing data \\(y\\), assuming we are at state \\(x\\). This is also called the generative model, as it describes how the evolution of the state can cause sensor measurements.</li> </ul> <p>Nice video explaining Bayes Theorem:</p>"},{"location":"State%20Estimation/State%20Estimation/","title":"State Estimation","text":"<p>Bayes Theorem [[Kalman Filter]] [[Extended Kalman Filter]] [[Unscented Kalman Filter]]</p> <p>%% Begin Waypoint %% - Bayes Theorem</p> <p>%% End Waypoint %%</p>"}]}