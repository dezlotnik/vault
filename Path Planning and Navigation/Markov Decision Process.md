---
id: Markov Decision Process
aliases:
  - Markov Decision Process
tags: []
---

# Markov Decision Process

A Markov Decision Process (MDP) is a model for sequential decision making when 
outcomes are uncertain.

An MDP is made up of four parts:
1. States
2. Actions
3. Transition function
4. Reward

For discrete states and actions the MDP can be represented as a [[Graph]].

## States

These are the nodes of the [[Graph]].

## Actions

Actions can be thought of as the edges in a traditional directed graph data
structure.

## Transition function

## Reward
